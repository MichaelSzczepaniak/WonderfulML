{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wonderful World of ML - Session 3 Assignment (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the references provided in session 2, I've uploading all the video and course notes from the [**Regression Models**](https://www.coursera.org/learn/regression-models) class from the Johns Hopkins Data Science Specialization on coursera to our meetup repo [here](https://github.com/focods/WonderfulML/tree/master/docs/JHU_DSS_RegMods).  This weeks assignment is adapted from an assignment given in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't done so by now, install jupyter notebook and configure it with an R kernel if you are an R user.  If you are Python user, your Anaconda install will have Python configured out of the box.  For further details, refer to the top of the session 2 notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only one problem for this session, but it's a good one..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What was the cost function Sondra mentioned that is used for linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this assignment can be found in our meetup repo:\n",
    "\n",
    "[https://raw.githubusercontent.com/focods/WonderfulML/master/data/mtcars.csv](https://raw.githubusercontent.com/focods/WonderfulML/master/data/mtcars.csv)\n",
    "\n",
    "**Here is the scenario:**  *You work for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:*\n",
    "\n",
    "1. Is an automatic or manual transmission better for MPG?  \n",
    "2. Quantify the MPG difference between automatic and manual transmissions.\n",
    "\n",
    "Here is some code to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Coming soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\betav}{\\mathbf{\\beta}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Sv}{\\mathbf{S}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\grad}{\\mathbf{\\nabla}}\n",
    "\\newcommand{\\ebx}[1]{e^{\\wv_{#1}^T \\xv_n}}\n",
    "\\newcommand{\\eby}[1]{e^{y_{n,#1}}}\n",
    "\\newcommand{\\Tiv}{\\mathbf{Ti}}\n",
    "\\newcommand{\\Fv}{\\mathbf{F}}\n",
    "\\newcommand{\\ones}[1]{\\mathbf{1}_{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wonderful World of ML - Session 3 Discussion: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MichaelSzczepaniak/WonderfulML/master/docs/graphics/child_on_logistic_curve_70per.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression provides a straightforward way to model the probability that one of two mutually exclusive outcomes will occur. This is another way of saying that this is a technique used for **binary classification**. Because this is one of the simplest and most popular classifiers out there, this should be in every machine learning practicioners toolbox.  It also serves as a good starting point for more sophisticated classification techniques such as LDA and QDA which we'll cover in the next session.\n",
    "\n",
    "What are some examples of binary classification?\n",
    "+ Will a team win or loss a game?\n",
    "+ Will a customer buy or not buy?\n",
    "+ Will a person survive 90 days after a myocardial infarction (heart attack)?\n",
    "+ Will a passenger survive the Titanic disaster? (kaggle competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is logistic regression modeling?\n",
    "\n",
    "At first look, it may seem odd to call this classifier logistic *regression* until you realize that this technique is modeling the underlying conditional probability of an event occuring given one or more predictors.  In other words, we are fitting a function to $p(C=k\\,|\\, \\xv_n)$ where $C=\\text{class of } n^\\text{th}\\text{ sample}$ which is binary,  meaning that k can only be 0 or 1.  The term $\\xv_n$ is a vector of predictors for the $n^{th}$ sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is this model constructed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural way to think about modeling a probablity is to divide the count of an event by the total number of events.  Let's start by proposing a function $f(\\xv;\\wv_k)$ that is proportional to the count or frequency of a given event $k$.  With our $f(\\xv;\\wv_k)$ as a function of our predictors $\\xv$ and some weight parameters $\\wv_k$ for each class, we can use this to compute probabilities by dividing $f(\\xv;\\wv_k)$ for a particular class $k$ by the total count in all classes which can be expressed as:\n",
    "\n",
    "1.$$\n",
    "    \\begin{align*}\n",
    "      p(C=k|\\xv) = \\frac{f(\\xv;\\wv_k)}{\\sum_{m=1}^K f(\\xv;\\wv_m)} = g_k(\\xv)\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "The above ensures that $p(C=k|\\xv)$ will be between 0 and 1 if $f(\\xv;\\wv_k) \\gt 0$, but we have another contraint which is:\n",
    "\n",
    "2.$$\n",
    "      \\begin{align*}\n",
    "      1  = \\sum_{k=1}^K p_k(C=k|\\xv) = \\sum_{k=1}^K g_k(\\xv)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "If we look closely at this equation 2., we notice that this really gives us only $(k-1)$ contraints on $g_k(\\xv)$ because we can always determine one these contraints by subtracting 1 from the sum of the other.  In other words, for some arbitrary $g_m(\\xv)$ where $m \\ne k$, we can write:\n",
    "\n",
    "3.$$\n",
    "      \\begin{align*}\n",
    "      g_m(\\xv) = 1 - \\sum_{k=1, k \\ne m}^K g_k(\\xv)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "So, we'll just set the final $f(\\xv;\\wv_k)$, for $k=K$, to be 1.  This gives us:\n",
    "\n",
    "4.$$\n",
    "      \\begin{align*}\n",
    "        g_k(\\xv) = \\left \\{ \\begin{array}{ll}\n",
    "            \\dfrac{f(\\xv;\\wv_k)}{1+\\sum_{m=1}^{K-1} f(\\xv;\\wv_m)}, & k < K\\\\\n",
    "            \\dfrac{1}{1+\\sum_{m=1}^{K-1} f(\\xv;\\wv_m)}, & k = K\n",
    "          \\end{array}\n",
    "        \\right .\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "Since we are only considering two classes, we can drop the subscript $k$ and write:\n",
    "\n",
    "5.$$\n",
    "      \\begin{align*}\n",
    "        g(\\xv) = \\dfrac{f(\\xv;\\wv)}{1+f(\\xv;\\wv)}\n",
    "      \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So how do we select $f(\\xv;\\wv)$?  As it turns out, selecting $f(\\xv;\\wv) = e^{\\wv^T \\xv}$ allows us to more easily evaluate the gradient of the log-likelihood function which is used to find the best fit parameters which we'll touch on in the next section below.  For now, we'll just point out that selecting our $f(\\xv;\\wv)$ in this way results in the *logisitic function* shown in equation (4.2) in the ISL and more generally below:\n",
    "\n",
    "6.$$\n",
    "      \\begin{align*}\n",
    "        g(\\xv) = \\dfrac{e^{\\wv^T \\xv}}{1+e^{\\wv^T \\xv}} = p(C=k|\\xv)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "If you play with 6. a little, you can easily convince yourself that this is the same function as $sigmoid(x) = \\dfrac{1}{1+e^{-\\wv^T \\xv}}$.  Leaving 6. in the form of the logistic function allows us to more easily see that with a little manipulation, we can rewrite 6. in terms of the *odds* or *odds ration* as:\n",
    "\n",
    "7.$$\n",
    "      \\begin{align*}\n",
    "        \\frac{p(C=k|\\xv)}{1 - p(C=k|\\xv)} = e^{\\wv^T \\xv} = \\text{odds or odds ratio}\n",
    "      \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we take the natural log of both sides of 7., we get a more general form of what is shown as equation (4.4) in the ISL.\n",
    "\n",
    "8.$$\n",
    "      \\begin{align*}\n",
    "        \\ln{\\Big(\\frac{p(C=k|\\xv)}{1 - p(C=k|\\xv)}\\Big)} = \\wv^T \\xv\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "The left side of 8. is referred to as the *log-odds* or *logit* and we can see clearly that the model assumes that this quantity is linear in the predictors $\\xv$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we find the best parameters for the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The most common way to solve for our parameters is to *maximize the likelihood* of our model fit to the data.  *What is the likelihood?*  The likelihood is just the product of all $p(C=\\text{class of }\n",
    "n^\\text{th}\\text{ sample}\\,|\\,\\xv_n)$ values for sample $n$.  A common way to express this product is\n",
    "\n",
    "9.$$\n",
    "    \\begin{align*}\n",
    "      L(\\wv) = \\prod_{n=1}^N \\prod_{k=1}^K p(C=k\\,|\\, \\xv_n)^{t_{n,k}}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "What's that $t_{n,k}$ term in 9. mean?  This is referred to as a *binary indicator variable* which can only take on the values of 1 or 0. $t_{n,k}$ is 1 when $k$ is the correct (target) class and 0 at all other values of $k$.  For example, say we have three classes ($K=3$) and training sample $n$ is from Class 2, then the  product is\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        p(C=1\\,|\\,\\xv_n)^{t_{n,1}} p(C=2\\,|\\,\\xv_n)^{t_{n,2}}\n",
    "        p(C=3\\,|\\,\\xv_n)^{t_{n,3}} & = \n",
    "         p(C=1\\,|\\,\\xv_n)^0 p(C=2\\,|\\,\\xv_n)^1 p(C=3\\,|\\,\\xv_n)^0 \\\\\n",
    "        & = 1\\; p(C=2\\,|\\,\\xv_n)^1 \\; 1 \\\\\n",
    "        & = p(C=2\\,|\\,\\xv_n) \n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "This shows how the indicator variables as exponents select the correct terms to be included in the product.\n",
    "\n",
    "Now that we know what our object function is, we can turn our attention to finding $\\wv$ that maximizes the data likelihood.  How should we proceed?\n",
    "\n",
    "Right.  Find the derivative of 9. with respect to each component of $\\wv$, or the gradient with respect to $\\wv$.  But there is a mess of products in this. So let's make this easier by working with the natural logarithm  $\\ln\\Big( L(\\wv)\\Big)$ which we will call $LL(\\wv)$.\n",
    "\n",
    "10.$$\n",
    "    \\begin{align*}\n",
    "      LL(\\wv) = \\ln\\Big( L(\\wv) \\Big) = \\sum_{n=1}^N \\sum_{k=1}^K t_{n,k}  \\log p(C=k\\,|\\,\\xv_n)\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Since $LL(\\wv)$ is not linear in $\\wv$, we need to solve 10. iteratively using gradient **ascent**, that is stepping up the gradient because we are **maximizing** our log-likelihood objective function as opposed to minimizing a *cost* function like we did for linear regression.\n",
    "\n",
    "The procedure is\n",
    "\n",
    "  * Initialize $\\wv$ to some value.\n",
    "  * Make small change to $\\wv$ in the direction of the  gradient of $LL(\\wv)$ with respect to $\\wv$  (or $\\grad_{\\wv} LL(\\wv)$)\n",
    "  * Repeat above step until $LL(\\wv)$ seems to be at a maximum.\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\wv \\leftarrow \\wv + \\alpha \\grad_{\\wv} LL(\\wv)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a constant that affects the step size.\n",
    "\n",
    "Deriving the expression for the gradient and the details regarding the update rules gets a bit involved, so for those that are interested, I describe these details in [this notebook (not available yet...)]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What tools are available in Python to do logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we handle discrete and continuous variables in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
