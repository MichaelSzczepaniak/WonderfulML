{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Wonderful World of ML - Session 3 Assignment (Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the references provided in session 2, I've uploading all the video and course notes from the [**Regression Models**](https://www.coursera.org/learn/regression-models) class from the Johns Hopkins Data Science Specialization on coursera to our meetup repo [here](https://github.com/focods/WonderfulML/tree/master/docs/JHU_DSS_RegMods).  This weeks assignment is adapted from an assignment given in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't done so by now, install jupyter notebook and configure it with an R kernel if you are an R user.  If you are Python user, your Anaconda install will have Python configured out of the box.  For further details, refer to the top of the session 2 notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only one problem for this session, but it's a good one..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What was the cost function Sondra mentioned that is used for linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this assignment can be found in our meetup repo:\n",
    "\n",
    "[https://raw.githubusercontent.com/focods/WonderfulML/master/data/mtcars.csv](https://raw.githubusercontent.com/focods/WonderfulML/master/data/mtcars.csv)\n",
    "\n",
    "**Here is the scenario:**  *You work for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:*\n",
    "\n",
    "1. Is an automatic or manual transmission better for MPG?  \n",
    "2. Quantify the MPG difference between automatic and manual transmissions.\n",
    "\n",
    "Here is some code to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
       "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mtcars = pd.read_csv(\"https://raw.githubusercontent.com/focods/WonderfulML/master/data/mtcars.csv\")\n",
    "mtcars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's do a little EDA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42592493/displaying-pair-plot-in-pandas-data-frame\n",
    "# Not as nice as the R Pairs plot from GGally library, but get's us going...\n",
    "#pd.plotting.scatter_matrix(mtcars, figsize=(15, 15), marker='o',\n",
    "#                           hist_kwds={'bins': 20}, s=60, alpha=.8);\n",
    "#plt.suptitle('Figure 1: Pairs Plot of Variables in mtcars Dataset',\n",
    "#             fontsize=18, fontweight='bold')\n",
    "#plt.show()\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"paper\", rc={\"axes.labelsize\":36, \"xtick.labelsize\":24, \"ytick.labelsize\":24})\n",
    "p1 = sns.pairplot(mtcars, kind='reg', diag_kind='hist') #, hue = 'am'),\n",
    "#                  vars = ['mpg', 'cyl', 'disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'gear', 'carb'])\n",
    "# remove upper triangular plots because they are redundant\n",
    "# https://stackoverflow.com/questions/34087126/plot-lower-triangle-in-a-seaborn-pairgrid\n",
    "for i, j in zip(*np.triu_indices_from(p1.axes, 1)):\n",
    "    p1.axes[i, j].set_visible(False)\n",
    "\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's take a look at the overall effect of transmission on MPG with this question in mind: *Is an automatic or manual transmission better for MPG?*  \n",
    "\n",
    "Let's start by taking a look at a box plot comparing all the automatics against all the manual transmissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto_cars = mtcars.loc[mtcars['am'] == 0, 'mpg']\n",
    "man_cars = mtcars.loc[mtcars['am'] == 1, 'mpg']\n",
    "sns.set_context(\"paper\", rc={\"axes.labelsize\":18})  # reset label size\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Figure 2: MPG by Transmission Only')\n",
    "ax = fig.add_subplot(111)\n",
    "# ; or plt.show() hides before plot output https://stackoverflow.com/questions/25790062/\n",
    "ax.boxplot([auto_cars, man_cars], labels=('0', '1'))\n",
    "\n",
    "ax.set_ylabel(\"mpg (miles per gallon)\")\n",
    "ax.set_xlabel(\"tranmission: 0 = automatic, 1 = manual\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot suggests that manuals get better mileage than automatics. Let's see if a t-test confirms this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as sp\n",
    "t, p = sp.ttest_ind(auto_cars, man_cars, equal_var=False)\n",
    "print(\"t = {}, p-value = {}\".format(t, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O.k., if we assume that other factors are **not** confounding our results, then the data suggests that manuals give better mileage.  So our next task is to check for factors that may be confounding the results keeping in mind that our goal is to create an *appropriate* linear model which focused on the effects of transmission (**am**), **not** to create a model that best predicts MPG from the dataset.\n",
    "\n",
    "An *appropriate* model for this goal was considered to be one that accurately quantified the effects of transmission on MPG (**mpg**) with only the required number of variables. Based on our initial assumption, the process of model selection should start by first building the simpliest possible model which is just the mean **mpg**.  This **f00** model will be our *reference*.\n",
    "\n",
    "The next simpliest model will again use **mpg** as the response, but include the **am** variable as a predictor.  This **f01** model will be our *base*.\n",
    "\n",
    "#### Important note here: We will use the entire dataset for this analysis because we are focusing on *characterization* of transmission on mpg rather than on prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "f00 = mtcars['mpg'].mean(axis=0)\n",
    "f01 = sm.OLS(mtcars['mpg'], sm.add_constant(mtcars['am']))\n",
    "res01 = f01.fit()\n",
    "print(res01.summary())  # same results as lm in R:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/292618/how-do-i-do-an-f-test-to-compare-nested-linear-models-in-python\n",
    "# Could write my own functions to do this, but would rather use existing api's..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\betav}{\\mathbf{\\beta}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Sv}{\\mathbf{S}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\grad}{\\mathbf{\\nabla}}\n",
    "\\newcommand{\\ebx}[1]{e^{\\wv_{#1}^T \\xv_n}}\n",
    "\\newcommand{\\eby}[1]{e^{y_{n,#1}}}\n",
    "\\newcommand{\\Tiv}{\\mathbf{Ti}}\n",
    "\\newcommand{\\Fv}{\\mathbf{F}}\n",
    "\\newcommand{\\ones}[1]{\\mathbf{1}_{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wonderful World of ML - Session 3 Discussion: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MichaelSzczepaniak/WonderfulML/master/docs/graphics/child_on_logistic_curve_70per.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression provides a straightforward way to model the probability that one of two mutually exclusive outcomes will occur. This is another way of saying that this is a technique used for **binary classification**. Because this is one of the simplest and most popular classifiers out there, this should be in every machine learning practicioners toolbox.  It also serves as a good starting point for more sophisticated classification techniques such as LDA and QDA which we'll cover in the next session.\n",
    "\n",
    "What are some examples of binary classification?\n",
    "+ Will a team win or loss a game?\n",
    "+ Will a customer buy or not buy?\n",
    "+ Will a person survive 90 days after a myocardial infarction (heart attack)?\n",
    "+ Will a passenger survive the Titanic disaster? (kaggle competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is logistic regression modeling?\n",
    "\n",
    "At first look, it may seem odd to call this classifier logistic *regression* until you realize that this technique is modeling the underlying conditional probability of an event occuring given one or more predictors.  In other words, we are fitting a function to $p(C=k\\,|\\, \\xv_n)$ where $C=\\text{class of } n^\\text{th}\\text{ sample}$ which is binary,  meaning that k can only be 0 or 1.  The term $\\xv_n$ is a vector of predictors for the $n^{th}$ sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is this model constructed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural way to think about modeling a probablity is to divide the count of an event by the total number of events.  Let's start by proposing a function $f(\\xv;\\wv_k)$ that is proportional to the count or frequency of a given event $k$.  With our $f(\\xv;\\wv_k)$ as a function of our predictors $\\xv$ and some weight parameters $\\wv_k$ for each class, we can use this to compute probabilities by dividing $f(\\xv;\\wv_k)$ for a particular class $k$ by the total count in all classes which can be expressed as:\n",
    "\n",
    "1.$$\n",
    "    \\begin{align*}\n",
    "      p(C=k|\\xv) = \\frac{f(\\xv;\\wv_k)}{\\sum_{m=1}^K f(\\xv;\\wv_m)} = g_k(\\xv)\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "The above ensures that $p(C=k|\\xv)$ will be between 0 and 1 if $f(\\xv;\\wv_k) \\gt 0$, but we have another contraint which is:\n",
    "\n",
    "2.$$\n",
    "      \\begin{align*}\n",
    "      1  = \\sum_{k=1}^K p_k(C=k|\\xv) = \\sum_{k=1}^K g_k(\\xv)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "If we look closely at this equation 2., we notice that this really gives us only $(k-1)$ contraints on $g_k(\\xv)$ because we can always determine one these contraints by subtracting 1 from the sum of the other.  In other words, for some arbitrary $g_m(\\xv)$ where $m \\ne k$, we can write:\n",
    "\n",
    "3.$$\n",
    "      \\begin{align*}\n",
    "      g_m(\\xv) = 1 - \\sum_{k=1, k \\ne m}^K g_k(\\xv)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "So, we'll just set the final $f(\\xv;\\wv_k)$, for $k=K$, to be 1.  This gives us:\n",
    "\n",
    "4.$$\n",
    "      \\begin{align*}\n",
    "        g_k(\\xv) = \\left \\{ \\begin{array}{ll}\n",
    "            \\dfrac{f(\\xv;\\wv_k)}{1+\\sum_{m=1}^{K-1} f(\\xv;\\wv_m)}, & k < K\\\\\n",
    "            \\dfrac{1}{1+\\sum_{m=1}^{K-1} f(\\xv;\\wv_m)}, & k = K\n",
    "          \\end{array}\n",
    "        \\right .\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "Since we are only considering two classes, we can drop the subscript $k$ and write:\n",
    "\n",
    "5.$$\n",
    "      \\begin{align*}\n",
    "        g(\\xv) = \\dfrac{f(\\xv;\\wv)}{1+f(\\xv;\\wv)}\n",
    "      \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So how do we select $f(\\xv;\\wv)$?  As it turns out, selecting $f(\\xv;\\wv) = e^{\\wv^T \\xv}$ allows us to more easily evaluate the gradient of the log-likelihood function which is used to find the best fit parameters which we'll touch on in the next section below.  For now, we'll just point out that selecting our $f(\\xv;\\wv)$ in this way results in the *logisitic function* shown in equation (4.2) in the ISL and more generally below:\n",
    "\n",
    "6.$$\n",
    "      \\begin{align*}\n",
    "        g(\\xv) = \\dfrac{e^{\\wv^T \\xv}}{1+e^{\\wv^T \\xv}} = p(C=k|\\xv)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "If you play with 6. a little, you can easily convince yourself that this is the same function as $sigmoid(x) = \\dfrac{1}{1+e^{-\\wv^T \\xv}}$.  Leaving 6. in the form of the logistic function allows us to more easily see that with a little manipulation, we can rewrite 6. in terms of the *odds* or *odds ration* as:\n",
    "\n",
    "7.$$\n",
    "      \\begin{align*}\n",
    "        \\frac{p(C=k|\\xv)}{1 - p(C=k|\\xv)} = e^{\\wv^T \\xv} = \\text{odds or odds ratio}\n",
    "      \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we take the natural log of both sides of 7., we get a more general form of what is shown as equation (4.4) in the ISL.\n",
    "\n",
    "8.$$\n",
    "      \\begin{align*}\n",
    "        \\ln{\\Big(\\frac{p(C=k|\\xv)}{1 - p(C=k|\\xv)}\\Big)} = \\wv^T \\xv\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "The left side of 8. is referred to as the *log-odds* or *logit* and we can see clearly that the model assumes that this quantity is linear in the predictors $\\xv$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we find the best parameters for the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The most common way to solve for our parameters is to *maximize the likelihood* of our model fit to the data.  *What is the likelihood?*  The likelihood is just the product of all $p(C=\\text{class of }\n",
    "n^\\text{th}\\text{ sample}\\,|\\,\\xv_n)$ values for sample $n$.  A common way to express this product is\n",
    "\n",
    "9.$$\n",
    "    \\begin{align*}\n",
    "      L(\\wv) = \\prod_{n=1}^N \\prod_{k=1}^K p(C=k\\,|\\, \\xv_n)^{t_{n,k}}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "What's that $t_{n,k}$ term in 9. mean?  This is referred to as a *binary indicator variable* which can only take on the values of 1 or 0. $t_{n,k}$ is 1 when $k$ is the correct (target) class and 0 at all other values of $k$.  For example, say we have three classes ($K=3$) and training sample $n$ is from Class 2, then the  product is\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        p(C=1\\,|\\,\\xv_n)^{t_{n,1}} p(C=2\\,|\\,\\xv_n)^{t_{n,2}}\n",
    "        p(C=3\\,|\\,\\xv_n)^{t_{n,3}} & = \n",
    "         p(C=1\\,|\\,\\xv_n)^0 p(C=2\\,|\\,\\xv_n)^1 p(C=3\\,|\\,\\xv_n)^0 \\\\\n",
    "        & = 1\\; p(C=2\\,|\\,\\xv_n)^1 \\; 1 \\\\\n",
    "        & = p(C=2\\,|\\,\\xv_n) \n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "This shows how the indicator variables as exponents select the correct terms to be included in the product.\n",
    "\n",
    "Now that we know what our object function is, we can turn our attention to finding $\\wv$ that maximizes the data likelihood.  How should we proceed?\n",
    "\n",
    "Right.  Find the derivative of 9. with respect to each component of $\\wv$, or the gradient with respect to $\\wv$.  But there is a mess of products in this. So let's make this easier by working with the natural logarithm  $\\ln\\Big( L(\\wv)\\Big)$ which we will call $LL(\\wv)$.\n",
    "\n",
    "10.$$\n",
    "    \\begin{align*}\n",
    "      LL(\\wv) = \\ln\\Big( L(\\wv) \\Big) = \\sum_{n=1}^N \\sum_{k=1}^K t_{n,k}  \\log p(C=k\\,|\\,\\xv_n)\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Since $LL(\\wv)$ is not linear in $\\wv$, we need to solve 10. iteratively using gradient **ascent**, that is stepping up the gradient because we are **maximizing** our log-likelihood objective function as opposed to minimizing a *cost* function like we did for linear regression.\n",
    "\n",
    "The procedure is\n",
    "\n",
    "  * Initialize $\\wv$ to some value.\n",
    "  * Make small change to $\\wv$ in the direction of the  gradient of $LL(\\wv)$ with respect to $\\wv$  (or $\\grad_{\\wv} LL(\\wv)$)\n",
    "  * Repeat above step until $LL(\\wv)$ seems to be at a maximum.\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\wv \\leftarrow \\wv + \\alpha \\grad_{\\wv} LL(\\wv)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a constant that affects the step size.\n",
    "\n",
    "Deriving the expression for the gradient and the details regarding the update rules gets a bit involved, so for those that are interested, I describe these details in [this notebook (not available yet...)]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What tools are available in Python to do logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For you Python folks, tool selection is a bit more complicated than for the R folks.  From my very limited research, the two more popular packages that folks tend to use are [scikit-learn](http://scikit-learn.org/stable/index.html) (sklearn package) and [statsmodels](http://www.statsmodels.org).  They each have their strengths and weakness as described [here for linear regression](https://becominghuman.ai/stats-models-vs-sklearn-for-linear-regression-f19df95ad99b).\n",
    "\n",
    "Sklearn implements **regularization** which we have not discussed yet, but is an important consideration.  There doesn't seem to be a way to turn off regularization directly through sklearn, but you can set the inverse regularization parameter to a very high value (e.g. 1e9) which should have the same effect as turning of regularization.  To understand why, [this video explains the general concept of regularization quite well](https://www.youtube.com/watch?v=KvtGD37Rm5I).  Keep in mind that this the example described here is for L2 regularization (aka *ridge* regression vs. lasso which is L1 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we handle discrete and continuous variables in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
