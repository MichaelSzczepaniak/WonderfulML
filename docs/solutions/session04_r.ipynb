{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wonderful World of ML - Session 4 Assignment: Logistic Regression & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Extend Logistic Regression Model From Session 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the solutions notebook for session 3, we ran a logistic regression on 2016 Broncos data.  We found that when we added the **Home** variable, the coefficient on this variable was not significantly different from 0, so we left it out of our model.\n",
    "\n",
    "In this problem, we're going to do a little feature engineering and add a variable called **YrdsDiff** which is computed by taking the difference of offensive yards minus defensive yards.  Try the following:\n",
    "\n",
    "1) Read in the revised dataset that includes offensive and defensive stats.  Treat all interger variables as continuous and rebuild the logistic regression model using just the Broncos scare and call this  model **logRegBroncos1**.\n",
    "\n",
    "2) Create a new column in the dataframe called **YrdsDiff** and populate that column with the difference of offensive yards minus defensive yards.  Build a new model using **DenWin** and the new **YrdsDiff** variable and call this model **logRegBroncos3**.\n",
    "\n",
    "3) Is the coefficient for the new **YrdsDiff** variable significantly different from 0 to justify adding it to our final model?  Why / Why not?\n",
    "\n",
    "4) What does the difference in the values for AIC for **logRegBroncos** and **logRegBroncos3** suggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Date</th><th scope=col>Week</th><th scope=col>DenScore</th><th scope=col>OppScore</th><th scope=col>DenWin</th><th scope=col>Home</th><th scope=col>Off1stDwns</th><th scope=col>OffPassYrds</th><th scope=col>OffRushYrds</th><th scope=col>Def1stDwns</th><th scope=col>DefPassYrds</th><th scope=col>DefRushYrds</th><th scope=col>Notes</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>09/08/2016</td><td>1         </td><td>21        </td><td>20        </td><td>1         </td><td>1         </td><td>21        </td><td>159       </td><td>148       </td><td>21        </td><td>176       </td><td>157       </td><td>          </td></tr>\n",
       "\t<tr><td>09/18/2016</td><td>2         </td><td>34        </td><td>20        </td><td>1         </td><td>1         </td><td>24        </td><td>266       </td><td>134       </td><td>19        </td><td>170       </td><td> 83       </td><td>          </td></tr>\n",
       "\t<tr><td>09/25/2016</td><td>3         </td><td>29        </td><td>17        </td><td>1         </td><td>0         </td><td>21        </td><td>303       </td><td> 52       </td><td>20        </td><td>189       </td><td>143       </td><td>          </td></tr>\n",
       "\t<tr><td>10/02/2016</td><td>4         </td><td>27        </td><td> 7        </td><td>1         </td><td>0         </td><td>22        </td><td>218       </td><td> 89       </td><td>15        </td><td>143       </td><td> 72       </td><td>          </td></tr>\n",
       "\t<tr><td>10/09/2016</td><td>5         </td><td>16        </td><td>23        </td><td>0         </td><td>1         </td><td>18        </td><td>183       </td><td> 84       </td><td>19        </td><td>250       </td><td>122       </td><td>          </td></tr>\n",
       "\t<tr><td>10/13/2016</td><td>6         </td><td>13        </td><td>21        </td><td>0         </td><td>0         </td><td>16        </td><td>220       </td><td> 84       </td><td>16        </td><td>166       </td><td> 99       </td><td>          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllll}\n",
       " Date & Week & DenScore & OppScore & DenWin & Home & Off1stDwns & OffPassYrds & OffRushYrds & Def1stDwns & DefPassYrds & DefRushYrds & Notes\\\\\n",
       "\\hline\n",
       "\t 09/08/2016 & 1          & 21         & 20         & 1          & 1          & 21         & 159        & 148        & 21         & 176        & 157        &           \\\\\n",
       "\t 09/18/2016 & 2          & 34         & 20         & 1          & 1          & 24         & 266        & 134        & 19         & 170        &  83        &           \\\\\n",
       "\t 09/25/2016 & 3          & 29         & 17         & 1          & 0          & 21         & 303        &  52        & 20         & 189        & 143        &           \\\\\n",
       "\t 10/02/2016 & 4          & 27         &  7         & 1          & 0          & 22         & 218        &  89        & 15         & 143        &  72        &           \\\\\n",
       "\t 10/09/2016 & 5          & 16         & 23         & 0          & 1          & 18         & 183        &  84        & 19         & 250        & 122        &           \\\\\n",
       "\t 10/13/2016 & 6          & 13         & 21         & 0          & 0          & 16         & 220        &  84        & 16         & 166        &  99        &           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Date | Week | DenScore | OppScore | DenWin | Home | Off1stDwns | OffPassYrds | OffRushYrds | Def1stDwns | DefPassYrds | DefRushYrds | Notes | \n",
       "|---|---|---|---|---|---|\n",
       "| 09/08/2016 | 1          | 21         | 20         | 1          | 1          | 21         | 159        | 148        | 21         | 176        | 157        |            | \n",
       "| 09/18/2016 | 2          | 34         | 20         | 1          | 1          | 24         | 266        | 134        | 19         | 170        |  83        |            | \n",
       "| 09/25/2016 | 3          | 29         | 17         | 1          | 0          | 21         | 303        |  52        | 20         | 189        | 143        |            | \n",
       "| 10/02/2016 | 4          | 27         |  7         | 1          | 0          | 22         | 218        |  89        | 15         | 143        |  72        |            | \n",
       "| 10/09/2016 | 5          | 16         | 23         | 0          | 1          | 18         | 183        |  84        | 19         | 250        | 122        |            | \n",
       "| 10/13/2016 | 6          | 13         | 21         | 0          | 0          | 16         | 220        |  84        | 16         | 166        |  99        |            | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Date       Week DenScore OppScore DenWin Home Off1stDwns OffPassYrds\n",
       "1 09/08/2016 1    21       20       1      1    21         159        \n",
       "2 09/18/2016 2    34       20       1      1    24         266        \n",
       "3 09/25/2016 3    29       17       1      0    21         303        \n",
       "4 10/02/2016 4    27        7       1      0    22         218        \n",
       "5 10/09/2016 5    16       23       0      1    18         183        \n",
       "6 10/13/2016 6    13       21       0      0    16         220        \n",
       "  OffRushYrds Def1stDwns DefPassYrds DefRushYrds Notes\n",
       "1 148         21         176         157              \n",
       "2 134         19         170          83              \n",
       "3  52         20         189         143              \n",
       "4  89         15         143          72              \n",
       "5  84         19         250         122              \n",
       "6  84         16         166          99              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = broncos_data$DenWin ~ broncos_data$DenScore, family = \"binomial\")\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.1947  -0.2699   0.2206   0.4761   1.2306  \n",
       "\n",
       "Coefficients:\n",
       "                      Estimate Std. Error z value Pr(>|z|)  \n",
       "(Intercept)            -7.0906     3.7653  -1.883   0.0597 .\n",
       "broncos_data$DenScore   0.3483     0.1702   2.047   0.0407 *\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 21.930  on 15  degrees of freedom\n",
       "Residual deviance: 10.965  on 14  degrees of freedom\n",
       "AIC: 14.965\n",
       "\n",
       "Number of Fisher Scoring iterations: 6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path <- \"https://raw.githubusercontent.com/MichaelSzczepaniak/WonderfulML/master/data/broncos2016.csv\"\n",
    "broncos_data <- read.csv(data_path)\n",
    "head(broncos_data)\n",
    "logRegBroncos <- glm(broncos_data$DenWin ~ broncos_data$DenScore, family=\"binomial\")\n",
    "summary(logRegBroncos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = broncos_data$DenWin ~ broncos_data$DenScore + broncos_data$YrdsDiff, \n",
       "    family = \"binomial\")\n",
       "\n",
       "Deviance Residuals: \n",
       "     Min        1Q    Median        3Q       Max  \n",
       "-1.49668  -0.07598   0.07638   0.35998   1.35019  \n",
       "\n",
       "Coefficients:\n",
       "                       Estimate Std. Error z value Pr(>|z|)\n",
       "(Intercept)           -15.94706   10.46565  -1.524    0.128\n",
       "broncos_data$DenScore   0.74469    0.47232   1.577    0.115\n",
       "broncos_data$YrdsDiff  -0.01815    0.01417  -1.281    0.200\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 21.9301  on 15  degrees of freedom\n",
       "Residual deviance:  8.6284  on 13  degrees of freedom\n",
       "AIC: 14.628\n",
       "\n",
       "Number of Fisher Scoring iterations: 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suppressMessages(suppressWarnings(library(dplyr)))\n",
    "suppressMessages(suppressWarnings(library(stats)))\n",
    "suppressMessages(suppressWarnings(library(base)))\n",
    "suppressMessages(suppressWarnings(library(bindrcpp)))\n",
    "\n",
    "library(dplyr)\n",
    "\n",
    "broncos_data <- mutate(broncos_data, Delta1stDwns = Off1stDwns - Def1stDwns)\n",
    "broncos_data <- mutate(broncos_data, YrdsOffTotal = OffPassYrds + OffRushYrds)\n",
    "broncos_data <- mutate(broncos_data, YrdsDefTotal = DefPassYrds + DefRushYrds)\n",
    "broncos_data <- mutate(broncos_data, YrdsDiff = YrdsOffTotal - YrdsDefTotal)\n",
    "#broncos_data[, c('DenWin', 'YrdsDiff')]\n",
    "#broncos_data\n",
    "logRegBroncos3 <- glm(broncos_data$DenWin ~ broncos_data$DenScore + broncos_data$YrdsDiff, family=\"binomial\")\n",
    "summary(logRegBroncos3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adding **YrdsDiff** is not significantly zero at 95% confidence, but the AIC is a little lower.  AIC is proxy for test error, so it suggests that **logRegBroncos3** might be slightly better than **logRegBroncos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Add an L2 Weight Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In session 3, we discussed how we could employ shrinkage methods to lower our risk of overfitting.  Specifically, we discussed the effects of applying L1 (lasso) and L2 (ridge) penalties to regression models.  What we did not discuss, was that you can apply them to classification.  The basic motivation behind applying these methods for classification are similar to those behind applying them for regression: We want to minimize overfitting.\n",
    "\n",
    "These 2 videos (in the repo) from the University of Washington's [**Machine Learning: Classification** class on coursera](https://www.coursera.org/learn/ml-classification/home/welcome) (requires an account on coursera) do a really nice job of describing why and how to apply an L2 weight penalty to a logistic regression model:\n",
    "\n",
    "+ [Penalizing large coefficients to mitigate overfitting](https://github.com/MichaelSzczepaniak/WonderfulML/raw/master/docs/resources/010%20-%20Penalizing%20large%20coefficients%20to%20mitigate%20overfitting.mp4)\n",
    "+ [L2 Regularized Logistic Regression](https://github.com/MichaelSzczepaniak/WonderfulML/raw/master/docs/resources/011%20-%20L2%20regularized%20logistic%20regression.mp4)\n",
    "\n",
    "Let's see how an L2 weight penalty can help us.  Try this task:\n",
    "\n",
    "1) Read in this cleaned and truncated version of the [Titanic data set](https://raw.githubusercontent.com/MichaelSzczepaniak/WonderfulML/master/data/titanic_class_sex_age.csv).  A description of the data can be found in the [Data Dictionary section of this document](https://github.com/MichaelSzczepaniak/Jamatitanic/blob/master/data/titanic%20data%20dictionary.pdf).  After the read, split the data into 3 partitions: 400 training sample, 200 test sample, and 114 validation samples.  Use the code provided below to get started.\n",
    "\n",
    "2) Using just the training set, fit a logistic regression model starting with **Pclass** then adding **Age** and **Sex** if they are significantly non-zero and improve AIC.  What does an improvement in AIC look like (increase or decrease)? Why?\n",
    "\n",
    "3) Use the training and validation set to determine which of these 10 values of L2 weight penalty $\\lambda$ minimizes the negative log-likelihood cost function (same as maximizing the log-likelihood).  Use the following algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>PassengerId</th><th scope=col>Survived</th><th scope=col>Pclass</th><th scope=col>Name</th><th scope=col>Sex</th><th scope=col>Age</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1                                                  </td><td>0                                                  </td><td>3                                                  </td><td>Braund, Mr. Owen Harris                            </td><td>male                                               </td><td>22                                                 </td></tr>\n",
       "\t<tr><td>2                                                  </td><td>1                                                  </td><td>1                                                  </td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female                                             </td><td>38                                                 </td></tr>\n",
       "\t<tr><td>3                                                  </td><td>1                                                  </td><td>3                                                  </td><td>Heikkinen, Miss. Laina                             </td><td>female                                             </td><td>26                                                 </td></tr>\n",
       "\t<tr><td>4                                                  </td><td>1                                                  </td><td>1                                                  </td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)       </td><td>female                                             </td><td>35                                                 </td></tr>\n",
       "\t<tr><td>5                                                  </td><td>0                                                  </td><td>3                                                  </td><td>Allen, Mr. William Henry                           </td><td>male                                               </td><td>35                                                 </td></tr>\n",
       "\t<tr><td>7                                                  </td><td>0                                                  </td><td>1                                                  </td><td>McCarthy, Mr. Timothy J                            </td><td>male                                               </td><td>54                                                 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " PassengerId & Survived & Pclass & Name & Sex & Age\\\\\n",
       "\\hline\n",
       "\t 1                                                   & 0                                                   & 3                                                   & Braund, Mr. Owen Harris                             & male                                                & 22                                                 \\\\\n",
       "\t 2                                                   & 1                                                   & 1                                                   & Cumings, Mrs. John Bradley (Florence Briggs Thayer) & female                                              & 38                                                 \\\\\n",
       "\t 3                                                   & 1                                                   & 3                                                   & Heikkinen, Miss. Laina                              & female                                              & 26                                                 \\\\\n",
       "\t 4                                                   & 1                                                   & 1                                                   & Futrelle, Mrs. Jacques Heath (Lily May Peel)        & female                                              & 35                                                 \\\\\n",
       "\t 5                                                   & 0                                                   & 3                                                   & Allen, Mr. William Henry                            & male                                                & 35                                                 \\\\\n",
       "\t 7                                                   & 0                                                   & 1                                                   & McCarthy, Mr. Timothy J                             & male                                                & 54                                                 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "PassengerId | Survived | Pclass | Name | Sex | Age | \n",
       "|---|---|---|---|---|---|\n",
       "| 1                                                   | 0                                                   | 3                                                   | Braund, Mr. Owen Harris                             | male                                                | 22                                                  | \n",
       "| 2                                                   | 1                                                   | 1                                                   | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female                                              | 38                                                  | \n",
       "| 3                                                   | 1                                                   | 3                                                   | Heikkinen, Miss. Laina                              | female                                              | 26                                                  | \n",
       "| 4                                                   | 1                                                   | 1                                                   | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female                                              | 35                                                  | \n",
       "| 5                                                   | 0                                                   | 3                                                   | Allen, Mr. William Henry                            | male                                                | 35                                                  | \n",
       "| 7                                                   | 0                                                   | 1                                                   | McCarthy, Mr. Timothy J                             | male                                                | 54                                                  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  PassengerId Survived Pclass\n",
       "1 1           0        3     \n",
       "2 2           1        1     \n",
       "3 3           1        3     \n",
       "4 4           1        1     \n",
       "5 5           0        3     \n",
       "6 7           0        1     \n",
       "  Name                                                Sex    Age\n",
       "1 Braund, Mr. Owen Harris                             male   22 \n",
       "2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 \n",
       "3 Heikkinen, Miss. Laina                              female 26 \n",
       "4 Futrelle, Mrs. Jacques Heath (Lily May Peel)        female 35 \n",
       "5 Allen, Mr. William Henry                            male   35 \n",
       "6 McCarthy, Mr. Timothy J                             male   54 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_all <- read.csv(\"https://raw.githubusercontent.com/MichaelSzczepaniak/WonderfulML/master/data/titanic_class_sex_age.csv\", na.strings = c(\"NA\", \"\"), stringsAsFactors = FALSE)\n",
    "data_all <- data_all[, c('PassengerId','Survived', 'Pclass', 'Name', 'Sex', 'Age')]\n",
    "head(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>400</li>\n",
       "\t<li>200</li>\n",
       "\t<li>114</li>\n",
       "\t<li>714</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 400\n",
       "\\item 200\n",
       "\\item 114\n",
       "\\item 714\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 400\n",
       "2. 200\n",
       "3. 114\n",
       "4. 714\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 400 200 114 714"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suppressMessages(suppressWarnings(library(caret)))\n",
    "suppressMessages(suppressWarnings(library(lattice)))\n",
    "suppressMessages(suppressWarnings(library(ggplot2)))\n",
    "suppressMessages(suppressWarnings(library(e1071)))\n",
    "\n",
    "library(caret)\n",
    "library(e1071)\n",
    "set.seed(711)\n",
    "\n",
    "# Factorize the discrete variables\n",
    "data_all$Survived <- factor(data_all$Survived)\n",
    "data_all$Sex <- factor(data_all$Sex)\n",
    "data_all$Pclass <- factor(data_all$Pclass)\n",
    "\n",
    "# Create training, validation, and test datasets\n",
    "train_test_indices <- sample(1:nrow(data_all), 600)\n",
    "valid_data <- data_all[-train_test_indices, ]     # 714 - 600 = 114 validation samples\n",
    "train_indices <- sample(train_test_indices, 400)  # 400 train samples\n",
    "test_indices <- setdiff(train_test_indices, train_indices)\n",
    "train_data <- data_all[train_indices, ]\n",
    "test_data <- data_all[test_indices, ]             # 200 test samples\n",
    "# Check: If train, test, and validation are disjoint, id_keys should have nrow(data_all) = 714 rows\n",
    "id_keys <- union(union(train_data$PassengerId, test_data$PassengerId), valid_data$PassengerId)\n",
    "c(nrow(train_data), nrow(test_data), nrow(valid_data), length(id_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Survived ~ Pclass, family = \"binomial\", data = train_data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-1.4028  -0.7059  -0.7059   0.9860   1.7389  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)   0.5158     0.1998   2.581  0.00984 ** \n",
       "Pclass2      -0.1863     0.2861  -0.651  0.51488    \n",
       "Pclass3      -1.7785     0.2641  -6.734 1.65e-11 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 543.58  on 399  degrees of freedom\n",
       "Residual deviance: 480.43  on 397  degrees of freedom\n",
       "AIC: 486.43\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_mod1 <- glm(Survived ~ Pclass, data=train_data, family=\"binomial\")\n",
    "summary(titanic_mod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significance of the intercept and **Pclass** slope look good.  Let's add **Age** and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Survived ~ Pclass + Age, family = \"binomial\", data = train_data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.1395  -0.8144  -0.5592   0.9203   2.4766  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)  2.270464   0.419155   5.417 6.07e-08 ***\n",
       "Pclass2     -0.520854   0.308600  -1.688   0.0915 .  \n",
       "Pclass3     -2.493670   0.321131  -7.765 8.15e-15 ***\n",
       "Age         -0.044378   0.008941  -4.963 6.92e-07 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 543.58  on 399  degrees of freedom\n",
       "Residual deviance: 452.79  on 396  degrees of freedom\n",
       "AIC: 460.79\n",
       "\n",
       "Number of Fisher Scoring iterations: 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_mod2 <- glm(Survived ~ Pclass + Age, data=train_data, family=\"binomial\")\n",
    "summary(titanic_mod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters appear significant and we got a small reduction in AIC, so it looks like we are going in the right direction.  Let's add our last parameter **Sex** and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Survived ~ Pclass + Age + Sex, family = \"binomial\", \n",
       "    data = train_data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.7544  -0.6141  -0.3442   0.5282   2.5527  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)  3.84730    0.54922   7.005 2.47e-12 ***\n",
       "Pclass2     -0.74882    0.37871  -1.977 0.048009 *  \n",
       "Pclass3     -2.54751    0.38901  -6.549 5.80e-11 ***\n",
       "Age         -0.03838    0.01034  -3.712 0.000206 ***\n",
       "Sexmale     -2.83001    0.29192  -9.695  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 543.58  on 399  degrees of freedom\n",
       "Residual deviance: 333.29  on 395  degrees of freedom\n",
       "AIC: 343.29\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_mod3 <- glm(Survived ~ Pclass + Age + Sex, data=train_data, family=\"binomial\")\n",
    "summary(titanic_mod3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters are still significant and we got an even bigger reduction in AIC, so let's see how this model does on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.815789473684211"
      ],
      "text/latex": [
       "0.815789473684211"
      ],
      "text/markdown": [
       "0.815789473684211"
      ],
      "text/plain": [
       "[1] 0.8157895"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retrain using caret interface for easier evaluation\n",
    "titanic_mod3 <- train(Survived ~ Pclass + Age + Sex, method='glm', data=train_data, family=\"binomial\")\n",
    "mod3_valid_results <- predict(titanic_mod3, newdata=valid_data)\n",
    "mod3_valid_acc <- sum(mod3_valid_results == valid_data$Survived) / nrow(valid_data)\n",
    "mod3_valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, now lets manually implement a search for an L2 weight penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\betav}{\\mathbf{\\beta}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Sv}{\\mathbf{S}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\grad}{\\mathbf{\\nabla}}\n",
    "\\newcommand{\\ebx}[1]{e^{\\wv_{#1}^T \\xv_n}}\n",
    "\\newcommand{\\eby}[1]{e^{y_{n,#1}}}\n",
    "\\newcommand{\\Tiv}{\\mathbf{Ti}}\n",
    "\\newcommand{\\Fv}{\\mathbf{F}}\n",
    "\\newcommand{\\ones}[1]{\\mathbf{1}_{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Wonderful World of ML - Session 4 Discussion: \n",
    "## Linear & Quadratic Discriminant Analysis (LDA & QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In logistic regression, we were fitting a function directly to our probablity of a certain class given the data or $P(C_n=k\\,|\\, \\xv_n)$ where $C_n=\\text{class of } n^\\text{th}\\text{ sample}$.  In LDA and QDA, we do something a little different.  We start by modeling the likelihood of the data within each class or $P(\\xv_n\\,|\\, C=k)$ and then use Bayes' Theorem (shown below) to compute the probability of a certain class given that data or $P(C_n=k\\,|\\, \\xv_n)$.\n",
    "\n",
    "(1)$$\n",
    "\tP(C=k|x) = \\frac{P(x|C=k) P(C=k)}{P(x)}\n",
    "$$\n",
    "\n",
    "This seems a little cumbersome doesn't it?  Can you think of any reasons why we would want to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Well, if we think about this a little, we might realize that we typically can observe how the data is distributed within a class (sometimes referred to as \"the likelihood of the data\") which is the $p(\\xv_n\\,|\\, C=k)$ term.  We might then conclude that it is natural to use these distributions within each class to infer the actual class from the data.\n",
    "\n",
    "In order to make use of (1), we need to make a important assumption about the probability distribution of a data sample from each class to define $P(\\xv_n\\,|\\, C=k)$.  A common assumption is that this distribution is Normal with mean $\\mu_k$ and covariance matrix $\\Sigma_k$.  This assumption allows $P(\\xv_n\\,|\\, C=k)$ to be expressed using the common relationship for the d-dimensional Guassian distribution as shown in equation (2).\n",
    "\n",
    "(2)$$\n",
    "P(x|C=k) = \\frac{1}{(2\\pi)^{\\frac{p}{2}} |\\Sigma_k|^{\\frac{1}{2}}}\n",
    "e^{-\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k)}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To classify $x$ as being from Class 1 in a two-class discrimination problem, we must check to see if  $P(C=1 | x) > P(C=2 | x)$ is true.  Rewriting each side of the inequality using Bayes' Theorem we get equation (3).\n",
    "\n",
    "(3)$$\n",
    "P(x|C=1) P(C=1) / P(x) > P(x|C=2)P(C=2)/P(x)\n",
    "$$\n",
    "\n",
    "Since $P(x)$ is positive, it can be removed from each side.  And since we have defined $P(x|C=k)$ to be a Normal distribution involving an exponential, we can take the logarithm and expand both sides as shown in equations (4) and (5).\n",
    "\n",
    "(4)$$\n",
    "\\log( P(x|C=1) P(C=1)) > \\log( P(x|C=2) P(C=2))\n",
    "$$\n",
    "\n",
    "(5)$$\n",
    "\\log( P(x|C=1)) + \\log( P(C=1)) > \\log( P(x|C=2)) + \\log( P(C=2))\n",
    "$$\n",
    "\n",
    "If we substitute equation (2) into (5) and simplify, the left side of the inequality would look like what is shown in (6) and the right side would look like what is shown in (7).\n",
    "\n",
    "(6)$$\n",
    "-\\frac{1}{2} \\log |\\Sigma_1| -\\frac{1}{2}(x-\\mu_1)^T \\Sigma_1^{-1} (x-\\mu_1) + \\log P(C=1)\n",
    "$$\n",
    "\n",
    "(7)$$\n",
    "-\\frac{1}{2} \\log |\\Sigma_2| -\\frac{1}{2}(x-\\mu_2)^T \\Sigma_2^{-1} (x-\\mu_2) + \\log P(C=2)\n",
    "$$\n",
    "\n",
    "If we define each side of this inequality as a discriminant function, $\\delta(x)$ for\n",
    "Class 1 or 2, then, in general\n",
    "\n",
    "(8)$$\n",
    "\\delta_k(x) = -\\frac{1}{2} \\log |\\Sigma_k| -\\frac{1}{2}(x-\\mu_k)^T\n",
    "\\Sigma_k^{-1} (x-\\mu_k) + \\log P(C=k)\n",
    "$$\n",
    "\n",
    "and the class of a new sample $x$ is $argmax_k \\delta_k(x)$.  Notice\n",
    "that the boundary between Class 1 and Class 2 is the set of points $x$\n",
    "for which $\\delta_1(x) = \\delta_2(x)$.  Substituting in the definitions of these discriminant functions we see that this equation is quadratic in $x$, meaning that the boundary between Class 1 and 2 is quadratic.  We have just defined **Quadratic Discriminant Analysis**, or **QDA**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply QDA to a given sets of data samples $X_1, X_2, \\ldots, X_K$ from Classes $1, 2, \\dots, K$, we must compute the likelihood and prior terms: $P(x|C=k)$ and $P(C=k)$ respectively, using (9), (10), and (11):\n",
    "\n",
    "(9)$$\n",
    "\\mu_k = \\frac{1}{N_k} \\sum_{x \\in X_k} x\n",
    "$$\n",
    "\n",
    "(10)$$\n",
    "\\Sigma_k = \\frac{1}{N_k-1} \\sum_{x\\in X_k} (x-\\mu_k) (x-\\mu_k)^T\n",
    "$$\n",
    "\n",
    "(11)$$\n",
    "P(C=k) = \\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "where $N_k$ is the number of samples in $X_k$ and $N$ is the number of all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is derived in the same way as QDA with the additional assumption that the variance within each class are equal.  Making this additional assumption allows us to simplify the terms in (6) and (7) defined on both sides inequality as shown in equation (12).\n",
    "\n",
    "(12)$$\n",
    "x^T \\Sigma^{-1} \\mu_1 - \\frac{1}{2}\\mu_1^T \\Sigma^{-1} \\mu_1 + \\log P(C=1) >\n",
    "x^T \\Sigma^{-1} \\mu_2 - \\frac{1}{2}\\mu_2^T \\Sigma^{-1} \\mu_2 + \\log P(C=2)\n",
    "$$\n",
    "\n",
    "which results in the new discriminant defined in equation (13).\n",
    "\n",
    "(13)$$\n",
    "\\delta_k(x) = \\frac{x^T \\mu_k}{\\Sigma} - \\frac{\\mu_k^T \\mu_k}{2 \\Sigma} + \\log P(C=k)\n",
    "$$\n",
    "\n",
    "where the new effectively averaged covariance is a scalar defined by (14).\n",
    "\n",
    "(14)$$\n",
    "\\Sigma = \\frac{1}{N-K} \\sum_{k=1}^K \\sum_{x\\in X_k}(x-\\mu_k)(x-\\mu_k)^T\n",
    "$$\n",
    "\n",
    "Like QDA, the maximum $\\delta_k(x)$ for a given $x$ determines the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement QDA and LDA on some 1 and 2-D sythetic data for illustration purposes.  After that, we'll take them for a spin on [an interesting real data set](http://archive.ics.uci.edu/ml/datasets/heart+Disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NULL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nRK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source('../../R/QdaLda.R')\n",
    "\n",
    "# Creates and returns a 1-D data table. nc=number of classes,\n",
    "# ms=the mean of the 1-D dataset, stds= std. deviation of the 1-D\n",
    "# data set.\n",
    "getData1D <- function(nc, ms, stds) {\n",
    "    data <- NULL\n",
    "\n",
    "    # create the data to classify\n",
    "    for (class in 1:nc) {\n",
    "        data <- rbind(data, cbind(rep(class, 10), rnorm(10,ms[class],stds)))\n",
    "    }\n",
    "    colnames(data) <- c(\"class\",\"sample\")\n",
    "\n",
    "    return(data)\n",
    "}\n",
    "\n",
    "data_1d <- getData1D(3, c(1, 2, 3), c(0.1, 0.1, 0.1))\n",
    "\n",
    "plot1D_Training <- function(dat=data_1d) {\n",
    "    plt <- plot(dat[,2],dat[,1],xlab=\"x (training data)\",ylab=\"class\",\n",
    "     main=c(\"Training Data with means=1,2,3\",\n",
    "            \"and standard deviation=0.1\"), axes=FALSE)\n",
    "    axis(1, seq(0,4,by=1.0))\n",
    "    axis(2, 1:4)\n",
    "    \n",
    "    return(plt)\n",
    "}\n",
    "\n",
    "plot1D_Training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
